{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1665373-5377-4f4a-8299-43c0b7d7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.integrate import simps\n",
    "from math import factorial\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "t = np.linspace(0, 0.004*35000, 35001)\n",
    "delta_t = 0.004\n",
    "n_values = np.arange(11)\n",
    "gauss_params = [5000, 0.25, 8e+00, 100, 0.2, 9, 50, 0.65, 10, 1.5e+00]\n",
    "model_params = [0.008, 11]\n",
    "\n",
    "\n",
    "#Define simulated data\n",
    "def skewed_IRF_with_small_peak(t, A1, sigma1, mu1, A2, sigma2, mu2, A3, sigma3, mu3, b):\n",
    "    # Main Gaussian peak\n",
    "    main_peak = A1 / (sigma1 * np.sqrt(2 * np.pi)) * np.exp(-((t - mu1)**2) / (2 * sigma1**2))\n",
    "    \n",
    "    # Smaller Gaussian peak to the right-hand side\n",
    "    small_peak = A2 / (sigma2 * np.sqrt(2 * np.pi)) * np.exp(-((t - mu2)**2) / (2 * sigma2**2))\n",
    "\n",
    "    smaller_peak =  A3 / (sigma3 * np.sqrt(2 * np.pi)) * np.exp(-((t - mu3)**2) / (2 * sigma3**2))\n",
    "    \n",
    "    # Combine the main peak, the small peak, and the baseline offset\n",
    "    irf = main_peak + small_peak +smaller_peak + b\n",
    "\n",
    "    return irf\n",
    "\n",
    "def skewed_IRF_with_small_peak_noise (t, A1, sigma1, mu1, A2, sigma2, mu2, A3, sigma3, mu3, b):\n",
    "    # Main Gaussian peak\n",
    "    main_peak = A1 / (sigma1 * np.sqrt(2 * np.pi)) * np.exp(-((t - mu1)**2) / (2 * sigma1**2))\n",
    "    \n",
    "    # Smaller Gaussian peak to the right-hand side\n",
    "    small_peak = A2 / (sigma2 * np.sqrt(2 * np.pi)) * np.exp(-((t - mu2)**2) / (2 * sigma2**2))\n",
    "\n",
    "    smaller_peak =  A3 / (sigma3 * np.sqrt(2 * np.pi)) * np.exp(-((t - mu3)**2) / (2 * sigma3**2))\n",
    "    \n",
    "    # Combine the main peak, the small peak, and the baseline offset\n",
    "    irf = main_peak + small_peak +smaller_peak + b\n",
    "    noise_level = 0.00005 * max(irf)\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=irf.shape)\n",
    "    return np.clip(irf + noise, a_min=0, a_max=None)\n",
    "\n",
    "#defined an IRF with noise as this is what you would experimentally measure as the IRF, will be used later\n",
    "\n",
    "def multi_exp_model_truncated(t, C, tau):\n",
    "    # Generate the model based on two exponential decays\n",
    "    model = C * np.exp(-t / tau) \n",
    "    # Perform convolution of the model with the input response function (irf)\n",
    "    raw_convolution = scipy.signal.fftconvolve(model, IRF_no_noise, mode='full')[:len(t)]\n",
    "    # Calculate noise level as a percentage of the maximum point of the raw convolution\n",
    "    noise_level = 0.0001 * max(raw_convolution)\n",
    "    # Generate random noise\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=raw_convolution.shape)\n",
    "    # Add noise to the raw convolution\n",
    "    noisy_convolution = raw_convolution + noise\n",
    "    max_idx = np.argmax(noisy_convolution)\n",
    "    t_truncated = t[max_idx:] - t[max_idx]  # Adjust time to start from zero at the max index\n",
    "    convoluted_truncated = noisy_convolution[max_idx:]\n",
    "    IRF_truncated = IRF_noisy[max_idx:]\n",
    "    return t_truncated, convoluted_truncated, IRF_truncated\n",
    "\n",
    "\n",
    "\n",
    "IRF_noisy = skewed_IRF_with_small_peak_noise(t, *gauss_params)\n",
    "IRF_no_noise = skewed_IRF_with_small_peak(t, *gauss_params)\n",
    "\n",
    "t_adjusted, simulated_data, IRF_adjusted = multi_exp_model_truncated(t, *model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e250e031-55f5-4c40-8a82-9757e11971f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104043.78640510264, 1091799.6338434655, 21896701.56436999, 621851452.916474, 21958765630.11307, 889084443967.8032, 37998690696527.0, 1427380096233197.0, 3742007942403491.0, -1.0468471146870245e+19, -2.14490957006832e+21]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definitions of your functions (unchanged)\n",
    "def single_exponential_approx(t, A, tau, c):\n",
    "    single_exp_decay_1 = A * np.exp(- (t / tau)) - c\n",
    "    raw_convolution = scipy.signal.fftconvolve(single_exp_decay_1, IRF_noisy, mode='full')[:len(t)]\n",
    "    return raw_convolution\n",
    "\n",
    "def single_exp_model_truncated(t, C, tau, c):\n",
    "    model = C * np.exp(-t / tau) - c\n",
    "    # Perform convolution of the model with the input response function (irf)\n",
    "    raw_convolution = scipy.signal.fftconvolve(model, IRF_noisy, mode='full')[:len(t)]\n",
    "    # Calculate noise level as a percentage of the maximum point of the raw convolution\n",
    "    noise_level = 0.0001 * max(raw_convolution)\n",
    "    # Generate random noise\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=raw_convolution.shape)\n",
    "    # Add noise to the raw convolution\n",
    "    noisy_convolution = raw_convolution + noise\n",
    "    max_idx = np.argmax(noisy_convolution)\n",
    "    t_truncated = t[max_idx:] - t[max_idx]  # Adjust time to start from zero at the max index\n",
    "    convoluted_truncated = noisy_convolution[max_idx:]\n",
    "    IRF_truncated = IRF_noisy[max_idx:]\n",
    "    return t_truncated, convoluted_truncated, IRF_truncated\n",
    "\n",
    "def Pure_single_exp(t, A, tau, c):\n",
    "    return A * np.exp(- (t / tau)) - c\n",
    "\n",
    "def binomial_coefficient(n, k):\n",
    "    # Calculate the binomial coefficient \"n choose k\"\n",
    "    return factorial(n) // (factorial(k) * factorial(n - k))\n",
    "\n",
    "def integrate_for_n(t, data, n):\n",
    "    # Original provided function to calculate the integral for the nth power\n",
    "    #background = 6.5\n",
    "    #background = np.mean(-data[int(0.0001*len(data)):])\n",
    "    #corrected_data = data - noise\n",
    "         # Calculate the background as mean of the last 10% of the data points\n",
    "    #n_background = round(len(data) * 0.4)\n",
    "    #background = np.mean(data[-n_background:])\n",
    "    fit_region = data[-int(0.451*len(data)):]\n",
    "    model = Polynomial.fit(t[-int(0.451*len(t)):], fit_region, deg=2)\n",
    "    background_fit = model(t)\n",
    "    corrected_data = data - background_fit \n",
    "    integrand_num = (t**n) * corrected_data\n",
    "    integrand_dem = corrected_data\n",
    "    integral_num = simps(integrand_num, t)\n",
    "    integral_dem = simps(integrand_dem, t)\n",
    "    return integral_num  \n",
    "\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "\n",
    "\n",
    "mu_k_values = [integrate_for_n(t_adjusted, simulated_data, n) for n in n_values]\n",
    "\n",
    "print(mu_k_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b432cc04-9e05-4c59-8a2c-52952a8133bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.83314671  0.81431556 -0.51405186 ... 32.58653002 32.35278928\n",
      " 33.59655344]\n",
      "[ 1.83314671  0.81431556  0.51405186 ... 32.58653002 32.35278928\n",
      " 33.59655344]\n"
     ]
    }
   ],
   "source": [
    "def multi_exp_model_fitting(t, C, tau):\n",
    "    # Generate the model based on two exponential decays\n",
    "    model = C * np.exp(-t / tau)\n",
    "    # Perform convolution of the model with the input response function (irf)\n",
    "    raw_convolution = scipy.signal.fftconvolve(model, IRF_no_noise, mode='full')[:len(t)]\n",
    "    # Calculate noise level as a percentage of the maximum point of the raw convolution\n",
    "    noise_level = 0.0001 * max(raw_convolution)\n",
    "    # Generate random noise\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=raw_convolution.shape)\n",
    "    # Add noise to the raw convolution\n",
    "    noisy_convolution = raw_convolution + noise\n",
    "    return noisy_convolution\n",
    "\n",
    "\n",
    "fitting_data =  multi_exp_model_fitting(t, *model_params)\n",
    "fitting_data_new = np.abs(fitting_data)\n",
    "print(fitting_data)\n",
    "print(fitting_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6beeaf3e-b252-4cb6-aab6-d9a9efffea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: A=58.83522296023517, tau=9.519306392855068\n",
      "Final RMSE: 1.7837801531399843e+24\n"
     ]
    }
   ],
   "source": [
    "# Initial fitting to get starting parameters\n",
    "Trial_single = [1, 1, 1e-3]\n",
    "\n",
    "popt, pcov = curve_fit(single_exponential_approx, t, fitting_data_new, p0=Trial_single)\n",
    "\n",
    "a_opt, tau_opt, c_opt = popt\n",
    "\n",
    "# Iterative process starts here\n",
    "max_iterations = 100\n",
    "tolerance = 1e-7\n",
    "previous_rmse = 6e30\n",
    "current_params = a_opt, tau_opt\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    # Use current parameters to calculate F_single\n",
    "    T_large = np.linspace(0.004*35000, 0.004*1000000, 965001)\n",
    "    F_single = Pure_single_exp(T_large, *current_params, c_opt)\n",
    "\n",
    "    # Calculate moments and new parameters\n",
    "    mu_0_calc = integrate_for_n(t_adjusted, simulated_data, 0) + integrate_for_n(T_large, F_single, 0)\n",
    "    m_0_calc = integrate_for_n(t_adjusted, IRF_adjusted, 0)\n",
    "    mu_1_calc = integrate_for_n(t_adjusted, simulated_data, 1) + integrate_for_n(T_large, F_single, 1)\n",
    "    m_1_calc = integrate_for_n(t_adjusted, IRF_adjusted, 1)\n",
    "    G1 = mu_0_calc / m_0_calc\n",
    "    G2 = (1 / m_0_calc)*(mu_1_calc - (mu_0_calc / m_0_calc)*(m_1_calc))\n",
    "    new_a = (G1**2) / G2\n",
    "    new_tau = G2 / G1\n",
    "    new_params = [new_a, new_tau]\n",
    "\n",
    "\n",
    "    t_adjusted_2, single_Data, irf = single_exp_model_truncated(t, *new_params, c_opt)\n",
    "\n",
    "    \n",
    "    # Compute mu_est_values for RMSE calculation\n",
    "    mu_est_values = [integrate_for_n(t_adjusted_2, single_Data, n) for n in n_values]\n",
    "    current_rmse = rmse(np.array(mu_est_values), np.array(mu_k_values))\n",
    "   \n",
    "    # Check for convergence\n",
    "    if abs(previous_rmse - current_rmse) < tolerance:\n",
    "        break\n",
    "    previous_rmse = current_rmse\n",
    "    current_params = new_params  # Update parameters for next iteration\n",
    "\n",
    "# Output the results\n",
    "print(f\"Optimized parameters: A={new_a}, tau={new_tau}\")\n",
    "print(f\"Final RMSE: {current_rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf1a5b4b-9e75-41f2-8ebc-231a1656c1b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge in Linear Least Squares",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 92\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     t_truncated_3, F_multi_approx, irf_3 \u001b[38;5;241m=\u001b[39m multi_exp_model_truncated_it(t, C_opt, result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m0\u001b[39m], result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m1\u001b[39m], result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m2\u001b[39m], result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m3\u001b[39m], c2_opt)\n\u001b[1;32m---> 92\u001b[0m     mu_est_values_multi \u001b[38;5;241m=\u001b[39m [integrate_for_n(t_truncated_3, F_multi_approx, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_values]\n\u001b[0;32m     93\u001b[0m     current_rmse_multi \u001b[38;5;241m=\u001b[39m rmse(np\u001b[38;5;241m.\u001b[39marray(mu_est_values_multi), np\u001b[38;5;241m.\u001b[39marray(mu_k_values))\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Check for convergence\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[67], line 92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     t_truncated_3, F_multi_approx, irf_3 \u001b[38;5;241m=\u001b[39m multi_exp_model_truncated_it(t, C_opt, result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m0\u001b[39m], result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m1\u001b[39m], result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m2\u001b[39m], result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m3\u001b[39m], c2_opt)\n\u001b[1;32m---> 92\u001b[0m     mu_est_values_multi \u001b[38;5;241m=\u001b[39m [integrate_for_n(t_truncated_3, F_multi_approx, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_values]\n\u001b[0;32m     93\u001b[0m     current_rmse_multi \u001b[38;5;241m=\u001b[39m rmse(np\u001b[38;5;241m.\u001b[39marray(mu_est_values_multi), np\u001b[38;5;241m.\u001b[39marray(mu_k_values))\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Check for convergence\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[50], line 39\u001b[0m, in \u001b[0;36mintegrate_for_n\u001b[1;34m(t, data, n)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintegrate_for_n\u001b[39m(t, data, n):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Original provided function to calculate the integral for the nth power\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#background = 6.5\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m#n_background = round(len(data) * 0.4)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m#background = np.mean(data[-n_background:])\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     fit_region \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.451\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(data)):]\n\u001b[1;32m---> 39\u001b[0m     model \u001b[38;5;241m=\u001b[39m Polynomial\u001b[38;5;241m.\u001b[39mfit(t[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.451\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(t)):], fit_region, deg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     40\u001b[0m     background_fit \u001b[38;5;241m=\u001b[39m model(t)\n\u001b[0;32m     41\u001b[0m     corrected_data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;241m-\u001b[39m background_fit \n",
      "File \u001b[1;32m~\\Downloads\\New folder\\Lib\\site-packages\\numpy\\polynomial\\_polybase.py:1037\u001b[0m, in \u001b[0;36mABCPolyBase.fit\u001b[1;34m(cls, x, y, deg, domain, rcond, full, w, window, symbol)\u001b[0m\n\u001b[0;32m   1034\u001b[0m     window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\n\u001b[0;32m   1036\u001b[0m xnew \u001b[38;5;241m=\u001b[39m pu\u001b[38;5;241m.\u001b[39mmapdomain(x, domain, window)\n\u001b[1;32m-> 1037\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(xnew, y, deg, w\u001b[38;5;241m=\u001b[39mw, rcond\u001b[38;5;241m=\u001b[39mrcond, full\u001b[38;5;241m=\u001b[39mfull)\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full:\n\u001b[0;32m   1039\u001b[0m     [coef, status] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\Downloads\\New folder\\Lib\\site-packages\\numpy\\polynomial\\polynomial.py:1362\u001b[0m, in \u001b[0;36mpolyfit\u001b[1;34m(x, y, deg, rcond, full, w)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolyfit\u001b[39m(x, y, deg, rcond\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, full\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;124;03m    Least-squares fit of a polynomial to data.\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \n\u001b[0;32m   1361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pu\u001b[38;5;241m.\u001b[39m_fit(polyvander, x, y, deg, rcond, full, w)\n",
      "File \u001b[1;32m~\\Downloads\\New folder\\Lib\\site-packages\\numpy\\polynomial\\polyutils.py:664\u001b[0m, in \u001b[0;36m_fit\u001b[1;34m(vander_f, x, y, deg, rcond, full, w)\u001b[0m\n\u001b[0;32m    661\u001b[0m scl[scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# Solve the least squares problem.\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m c, resids, rank, s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mlstsq(lhs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m/\u001b[39mscl, rhs\u001b[38;5;241m.\u001b[39mT, rcond)\n\u001b[0;32m    665\u001b[0m c \u001b[38;5;241m=\u001b[39m (c\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m/\u001b[39mscl)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# Expand c to include non-fitted coefficients which are set to zero\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\New folder\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2326\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(a, b, rcond)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_rhs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2324\u001b[0m     \u001b[38;5;66;03m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m     b \u001b[38;5;241m=\u001b[39m zeros(b\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (m, n_rhs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 2326\u001b[0m x, resids, rank, s \u001b[38;5;241m=\u001b[39m gufunc(a, b, rcond, signature\u001b[38;5;241m=\u001b[39msignature, extobj\u001b[38;5;241m=\u001b[39mextobj)\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2328\u001b[0m     x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\Downloads\\New folder\\Lib\\site-packages\\numpy\\linalg\\linalg.py:124\u001b[0m, in \u001b[0;36m_raise_linalgerror_lstsq\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_lstsq\u001b[39m(err, flag):\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge in Linear Least Squares\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: SVD did not converge in Linear Least Squares"
     ]
    }
   ],
   "source": [
    "\n",
    "#fit is not getting any better therefore we add one component\n",
    "\n",
    "#define new double exp model\n",
    "def Pure_multi_exp(t, C, a1, tau1, a2, tau2, c):\n",
    "    a2 = 1 - a1\n",
    "    return C*(a1 * np.exp(- (t / tau1)) + a2 * np.exp(- (t / tau2))) - c\n",
    "   \n",
    "def multi_exp_model_truncated_it(t, C, a1, tau1, a2, tau2, c):\n",
    "    # Generate the model based on two exponential decays\n",
    "    model = C * (a1 * np.exp(-t / tau1) + a2 * np.exp(-t / tau2)) - c\n",
    "    # Perform convolution of the model with the input response function (irf)\n",
    "    raw_convolution = scipy.signal.fftconvolve(model, IRF_noisy, mode='full')[:len(t)]\n",
    "    # Calculate noise level as a percentage of the maximum point of the raw convolution\n",
    "    noise_level = 0.0001 * max(raw_convolution)\n",
    "    # Generate random noise\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=raw_convolution.shape)\n",
    "    # Add noise to the raw convolution\n",
    "    noisy_convolution = raw_convolution + noise\n",
    "    max_idx = np.argmax(noisy_convolution)\n",
    "    t_truncated = t[max_idx:] - t[max_idx]  # Adjust time to start from zero at the max index\n",
    "    convoluted_truncated = noisy_convolution[max_idx:]\n",
    "    IRF_truncated = IRF_noisy[max_idx:]\n",
    "    return t_truncated, convoluted_truncated, IRF_truncated\n",
    "\n",
    "def multi_exp_model_it(t, C, a1, tau1, tau2, c):\n",
    "    a2 = 1 - a1\n",
    "    # Generate the model based on two exponential decays\n",
    "    model = C * (a1 * np.exp(-t / tau1) + a2 * np.exp(-t / tau2)) - c\n",
    "    # Perform convolution of the model with the input response function (irf)\n",
    "    raw_convolution = scipy.signal.fftconvolve(model, IRF_noisy, mode='full')[:len(t)]\n",
    "    # Calculate noise level as a percentage of the maximum point of the raw convolution\n",
    "    noise_level = 0.0001 * max(raw_convolution)\n",
    "    # Generate random noise\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=raw_convolution.shape)\n",
    "    # Add noise to the raw convolution\n",
    "    noisy_convolution = raw_convolution + noise\n",
    "    return noisy_convolution\n",
    "\n",
    "\n",
    "Trial_double = [1, 1, 1, 1, 1e-3]\n",
    "bounds = ([0, 0, 0, 0, 0], [np.inf, 1, np.inf, np.inf, np.inf]) \n",
    "popt_2, pcov_2 = curve_fit(multi_exp_model_it, t, fitting_data_new, p0=Trial_double, bounds=bounds)\n",
    "C_opt, a1_opt, tau1_opt, tau2_opt, c2_opt = popt_2\n",
    "\n",
    "a2_opt = 1 - a1_opt\n",
    "\n",
    "max_iterations_multi = 100\n",
    "current_params_multi = popt_2\n",
    "tolerance_multi = 1e-6\n",
    "previous_rmse_multi = 6e30\n",
    "\n",
    "def residuals(vars, G1, G2, G3, G4):\n",
    "    a1, tau1, a2, tau2 = vars[0], vars[1], vars[2], vars[3]\n",
    "    return [\n",
    "        a1 * tau1 + a2 * tau2 - G1,\n",
    "        a1 * tau1**2 + a2 * tau2**2 - G2,\n",
    "        a1 * tau1**3 + a2 * tau2**3 - G3,\n",
    "        a1 * tau1**4 + a2 * tau2**4 - G4\n",
    "    ]\n",
    "\n",
    "for iteration in range(max_iterations_multi):\n",
    "    # Use current parameters to calculate F_single\n",
    "    T_large = np.linspace(0.004*35000, 0.004*1000000, 965001)\n",
    "    F_Multi = Pure_multi_exp(T_large, C_opt, a1_opt, tau1_opt, a2_opt, tau2_opt, c2_opt)\n",
    "\n",
    "    # Calculate moments and new parameters\n",
    "    mu_0_calc = integrate_for_n(t_adjusted, simulated_data, 0) + integrate_for_n(T_large, F_Multi, 0)\n",
    "    m_0_calc = integrate_for_n(t_adjusted, IRF_adjusted, 0)\n",
    "    mu_1_calc = integrate_for_n(t_adjusted, simulated_data, 1) + integrate_for_n(T_large, F_Multi, 1)\n",
    "    m_1_calc = integrate_for_n(t_adjusted, IRF_adjusted, 1)\n",
    "    mu_2_calc = integrate_for_n(t_adjusted, simulated_data, 2) + integrate_for_n(T_large, F_Multi, 2)\n",
    "    m_2_calc = integrate_for_n(t_adjusted, IRF_adjusted, 2)\n",
    "    mu_3_calc = integrate_for_n(t_adjusted, simulated_data, 3) + integrate_for_n(T_large, F_Multi, 3)\n",
    "    m_3_calc = integrate_for_n(t_adjusted, IRF_adjusted, 3)\n",
    "\n",
    "    G1 = mu_0_calc / m_0_calc\n",
    "    G2 = (1 / m_0_calc)*(mu_1_calc - G1*m_1_calc)\n",
    "    G3 = (1 / m_0_calc)*((mu_2_calc / 2) - (G1*m_2_calc / 2) - G2*m_1_calc)\n",
    "    G4 = (1 / m_0_calc)*((mu_3_calc / 6) - (G1*m_3_calc / 6) - (G2*m_2_calc / 2) -G3*mu_1_calc)\n",
    "\n",
    "    params = a1_opt, tau1_opt, a2_opt, tau2_opt\n",
    "\n",
    "\n",
    "    \n",
    "# Perform least squares optimization\n",
    "    result = least_squares(residuals, params, args=(G1, G2, G3, G4))\n",
    "    \n",
    "# Display the results\n",
    "\n",
    "    t_truncated_3, F_multi_approx, irf_3 = multi_exp_model_truncated_it(t, C_opt, result.x[0], result.x[1], result.x[2], result.x[3], c2_opt)\n",
    "   \n",
    "    mu_est_values_multi = [integrate_for_n(t_truncated_3, F_multi_approx, n) for n in n_values]\n",
    "    current_rmse_multi = rmse(np.array(mu_est_values_multi), np.array(mu_k_values))\n",
    "\n",
    "    # Check for convergence\n",
    "    if abs(previous_rmse_multi - current_rmse_multi) < tolerance_multi:\n",
    "        break\n",
    "    previous_rmse_multi = current_rmse_multi\n",
    "    a1_opt, tau1_opt, a2_opt, tau2_opt = result.x  # Update parameters for next iteration\n",
    "\n",
    "# Output the results\n",
    "print(result.x)\n",
    "print(f\"Final RMSE: {current_rmse_multi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28afb7-6a22-4182-a1d3-a14706d906b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
